# Entrainement d'un CNN pour une tÃ¢che de reconnaissance d'animaux

## Description des fichiers
D'abord, le fichier **findsize.py** permet de trouver la gamme des dimensions des images dans les donnÃ©es afin de savoir quel padding est nÃ©cessaire.
Ensuite, le fichier **preprocess.py** effectue le prÃ©traitement appropriÃ© des donnÃ©es (les tranformant en images 256x256 en employant le padding.
L'entrainement du modÃ¨le se fait sur les donnÃ©es prÃ©traitÃ©es grÃ¢ce au fichier **1_Modele.py**, et l'Ã©valuation se fait dans le fichier **2_Evaluation.py**.

Le fichier **Philip_Voinea_TP2.pdf** contient le raport complÃ¨t du projet, dont vous trouverez le contenu ci-dessous.

# Construction dâ€™un modÃ¨le de classification dâ€™images dâ€™animaux

## ğŸ› ï¸ Montage de lâ€™architecture et entrainement du modÃ¨le

### Ensemble de donnÃ©es

Lâ€™ensemble de donnÃ©es dâ€™entrainement fournies constitue 24000 images dâ€™animaux (4000 pour chacune des six classes dâ€™animaux : baleine, dauphin, morse, phoque, requin et requin baleine). Jâ€™ai rÃ©servÃ© 20% de cet ensemble de cÃ´tÃ© pour la validation, câ€™est-Ã -dire 4800 images (800 par classe).

Lâ€™ensemble de donnÃ©es de test constitue 6000 images dâ€™animaux, soit 1000 pour chacune des six classes.

### Traitement des donnÃ©es

Les dimensions des images varient (la longueur dâ€™un cÃ´tÃ© varie dâ€™un minimum de 35 pixels Ã  un maximum de 256 pixels); mais vu que lâ€™entrÃ©e de lâ€™architecture CNN a une taille fixe, un ajustement est donc nÃ©cessaire pour uniformiser les dimensions des images. Ã€ cette fin, jâ€™ai appliquÃ© un padding sur les images en collant chaque image au centre dâ€™un arriÃ¨re-plan noir de 256x256 pixels, ce qui correspond aux dimensions maximales de lâ€™ensemble des donnÃ©es. Je nâ€™ai pas appliquÃ© de redimensionnement ou de zoom afin de garder les images aussi fidÃ¨les que possibles aux images sources.

Jâ€™ai commencÃ© par le data augmentation qui existait dans le code de base Ã  implÃ©menter du data augmentation quand jâ€™ai remarquÃ© que mes modÃ¨les surapprenaient les donnÃ©es dâ€™entrainement sans amÃ©lioration sur la validation de la maniÃ¨re suivante :
- shear_range=0.1 (cisaillement de lâ€™image de -0.1 Ã  0.1 degrÃ©s)
- zoom_range=0.1 (zoom de -10% Ã  10%)
- rotation_range=30 (rotation de -30 Ã  30 degrÃ©s)
- brightness_range=(0.7,1.3) (luminositÃ© de 70% Ã  130% de lâ€™original) 
- channel_shift_range=30 (dÃ©calage des canaux RVB de -30 Ã  +30)
- height_shift_range=0.1 (dÃ©calage vertical de lâ€™image de -10% Ã  10%)
- width_shift_range=0.1 (dÃ©calage horizontal de lâ€™image de -10% Ã  10%)
- horizontal_flip=True (reflection horizontale alÃ©atoire de lâ€™image)

### ParamÃ¨tres et hyperparamÃ¨tres

Lâ€™optimisateur Adam avec les valeurs attribuÃ©es par dÃ©faut dans Keras (Î± = 0.001, Î²1 = 0.9, Î²2 = 0.999, Îµ = 10^-7) mâ€™a suffi. Jâ€™utilise lâ€™entropie croisÃ©e comme fonction de perte.

Pour lâ€™entrainement, jâ€™ai utilisÃ© des lots de 64, soit la taille maximale de lot que les GPU T4 de Google Colab peuvent supporter.

Jâ€™ai entrainÃ© le modÃ¨le pendant 100 Ã©poques sans arrÃªt prÃ©coce automatique.

La raison pour laquelle je nâ€™ai pas implÃ©mentÃ© dâ€™arrÃªt prÃ©coce automatique est que le risque me paraissait trop Ã©levÃ©; en effet, mes modÃ¨les passaient parfois nombreuses Ã©poques Ã  stagner avant dâ€™amÃ©liorer lâ€™exactitude sur les donnÃ©es de validation, souvent parce quâ€™une Ã©poque passÃ©e avait trop bien performÃ© sur les donnÃ©es de validation, peut-Ãªtre par chance. Ã‰tant donnÃ© ces oscillations chaotiques dâ€™Ã©poque en Ã©poque dans la performance sur les donnÃ©es de validation, jâ€™ai jugÃ© que ce serait mieux dâ€™arrÃªter manuellement lâ€™entrainement lorsque les exactitudes tombent ou stagnent en moyenne. Jâ€™aurais pu implÃ©menter ou dÃ©velopper un algorithme dâ€™arrÃªt prÃ©coce basÃ© sur une moyenne mobile comme critÃ¨re dâ€™arrÃªt, mais je me suis contentÃ© de le faire manuellement et Ã  lâ€™Å“il Ã©tant donnÃ© le petit nombre de modÃ¨les que jâ€™ai dÃ» tester.

### Architecture

Les entrÃ©es passent par trois blocs de convolution avant de passer par trois couches complÃ¨tement connectÃ©es et la couche de sortie de 6 neurones. Jâ€™utilise du drop-out avec un taux dâ€™extinction de 0.2 et toutes les fonctions dâ€™activation sont ReLU. Jâ€™utilise Ã©galement de la rÃ©gularisation L1L2 sur les couches denses avec l1 = 10^-5 et l2 = 10^-4.

Premier bloc de convolution:
-	Deux couches de convolution 2D avec 32 filtres chacune de taille 3x3, avec un remplissage zero padding (padding = 'same') pour conserver la taille de lâ€™image et une activation ReLU.
-	Une normalisation par lot (BatchNormalization).
-	Une couche de sous-Ã©chantillonnage (MaxPooling2D) avec une fenÃªtre de 2Ã—2 et un remplissage zero padding (padding = 'same').
-	Un dropout avec un taux dâ€™extinction de 0.2.
  
DeuxiÃ¨me bloc de convolution:
-	Deux couches de convolution 2D avec 64 filtres chacune de taille 3x3, avec un remplissage zero padding (padding = 'same') pour conserver la taille de lâ€™image et une activation ReLU.
-	Une normalisation par lot (BatchNormalization).
-	Une couche de sous-Ã©chantillonnage (MaxPooling2D) avec une fenÃªtre de 2Ã—2 et un remplissage zero padding (padding = 'same').
-	Un dropout avec un taux dâ€™extinction de 0.2.
  
TroisiÃ¨me bloc de convolution:
-	Deux couches de convolution 2D avec 128 filtres chacune de taille 3x3, avec un remplissage zero padding (padding = 'same') pour conserver la taille de lâ€™image et une activation ReLU.
-	Une normalisation par lot (BatchNormalization).
-	Une couche de sous-Ã©chantillonnage (MaxPooling2D) avec une fenÃªtre de 2Ã—2 et un remplissage zero padding (padding = 'same').
-	Un dropout avec un taux dâ€™extinction de 0.2.
Partie entiÃ¨rement connectÃ©e (fonction fully_connected) :
-	La couche Â« Flatten Â» convertit les matrices obtenues en un vecteur de longueur 131072.
-	Une couche dense de 512 neurones avec fonction dâ€™activation ReLU et rÃ©gularisation L1L2 (l1=1e-5 et l2=1e-4).
-	Un dropout avec un taux dâ€™extinction de 0.2.
-	Une couche dense de 256 neurones avec fonction dâ€™activation ReLU et rÃ©gularisation L1L2 (l1=1e-5 et l2=1e-4).
-	Un dropout avec un taux dâ€™extinction de 0.2.
-	Une couche dense de sortie Ã  6 neurones avec fonction dâ€™activation softmax.

<img width="1721" height="586" alt="image" src="https://github.com/user-attachments/assets/4ae56471-d38f-4a84-b4af-1594d1587074" />

### RÃ©sultats dâ€™entrainement

Lâ€™entrainement a durÃ© environ 1 heure et 53 minutes.

Lâ€™erreur minimale commise lors de lâ€™entrainement fut de 0.9746 sur les donnÃ©es dâ€™entrainement et 0.9675 sur les donnÃ©es de validation.
Lâ€™exactitude maximale atteinte lors de lâ€™entrainement fut 0.8603 sur les donnÃ©es dâ€™entrainement (augmentÃ©es) et 0.8698 sur les donnÃ©es de validation.

<img width="500" alt="image" src="https://github.com/user-attachments/assets/384038e3-c172-4415-ad2e-f1c30a445a10" />
<img width="500" alt="image" src="https://github.com/user-attachments/assets/6cd5d2bc-fff0-46f3-afa0-4813c4ad7bdc" />

### Justification du choix de lâ€™architecture

1.	**Le squelette**

Je me suis inspirÃ© de lâ€™architecture fournie par lâ€™exemple MNIST. Dâ€™abord, jâ€™ai seulement adaptÃ© la couche de sortie pour un problÃ¨me Ã  6 classes et il Ã©tait clair que le problÃ¨me Ã©tait bien trop petit.

Jâ€™ai donc essayÃ© deux approches diffÃ©rentes pour complexifier le modÃ¨le : Ã©largir et approfondir les couches complÃ¨tement connectÃ©es et ajouter des couches de convolution.

Jâ€™ai ajoutÃ© une troisiÃ¨me couche de convolution avec 128 filtres, soit le double du nombre de filtres dans la couche de convolution prÃ©cÃ©dente parce que jâ€™ai remarquÃ© que la deuxiÃ¨me couche de convolution dans lâ€™exemple contenait le double du nombre de filtres de la premiÃ¨re couche de convolution. Jâ€™ai recherchÃ© cette architecture et jâ€™ai lu quâ€™il est commun dâ€™augmenter le nombre de filtres dans les couches de convolution subsÃ©quentes parce que tandis que les premiÃ¨res couches tendent Ã  capturer les caractÃ©ristiques de bas-niveau (des lignes, des coins, des bordures, etc.), les caractÃ©ristiques de haut-niveau captÃ©es par les derniÃ¨res convolutions sont plus complexes et diverses et bÃ©nÃ©ficient donc de plus de filtres, ce qui correspond avec mes connaissances prÃ©alables en CNN.

Jâ€™ai ensuite doublÃ© les couches de convolution afin dâ€™augmenter le champ rÃ©ceptif des convolutions, premiÃ¨rement parce que je doutais bien que les caractÃ©ristiques importantes dans des images dâ€™animaux de dimensions 256x256 ne requiÃ¨rent pas un plus grand champ rÃ©ceptif que les caractÃ©ristiques de chiffres dans des boÃ®tes 28x28. Mais plutÃ´t que de garder le mÃªme nombre de convolutions et dâ€™agrandir leur champ rÃ©ceptif Ã  5x5, jâ€™ai dÃ©cidÃ© de doubler les couches de convolution 3x3; jâ€™ai vu cette pratique dans des architectures CNN et jâ€™ai lu que deux convolutions 3x3 requiÃ¨rent moins dâ€™hyperparamÃ¨tres quâ€™une convolution 5x5 et que la non-linÃ©aritÃ© additionnelle peut Ãªtre avantageuse (Simonyan & Zisserman, 2015). Jâ€™ai donc essayÃ© cette mÃ©thode et cela a beaucoup amÃ©liorÃ© mes rÃ©sultats.

Finalement, jâ€™ai choisi la fonction dâ€™activation ReLU pour toutes mes couches afin dâ€™Ã©viter des problÃ¨mes de gradient (vanishing and exploding gradient) et de simplifier les calculs. Jâ€™ai considÃ©rÃ© la fonction LeakyReLU, qui me semblait auparavant nettement supÃ©rieure Ã©tant donnÃ© que les neurones ne peuvent pas mourir, mais le fait que la fonction ReLU demeure toujours plus populaire, dâ€™aprÃ¨s le cours, mâ€™a intriguÃ©. Cela mâ€™a poussÃ© Ã  rechercher les avantages de ReLU sur LeakyReLU et jâ€™ai appris que la mort de neurones peut constituer un avantage car cela encourage les reprÃ©sentations parsimonieuses et peut donc rÃ©duire le overfitting. Jâ€™ai donc gardÃ© ReLU comme fonction dâ€™activation Ã  travers lâ€™architecture.

2.	**La rÃ©gularization**

Jâ€™ai implÃ©mentÃ© un dropout avec un taux dâ€™extinction minimal de 0.2 sur les couches denses parce que jâ€™ai remarquÃ© que la performance de mon modÃ¨le sur les donnÃ©es de validation stagnait mÃªme quand lâ€™exactitude sur les donnÃ©es dâ€™entrainement augmentait. Ce dropout a rÃ©duit le surapprentissage dont mon modÃ¨le souffrait, mais pas assez. Jâ€™ai donc testÃ© trois mesures additionnelles en parallel : augmenter le taux dâ€™extinction (Ã  0.3 et ensuite Ã  0.5), implÃ©menter la rÃ©gularization L1L2 sur les couches denses (avec les valeurs de dÃ©faut de Keras, l1=l2=0.01) et appliquer le dropout aux blocs de convolution aussi. Les deux premiÃ¨res mesures ont trop obstruÃ© lâ€™apprentissage mÃªme des donnÃ©es dâ€™entrainement, tandis que la derniÃ¨re mesure a bien marchÃ©.

Jâ€™ai ensuite appliquÃ© une trÃ¨s lÃ©gÃ¨re rÃ©gularization L1L2 (l1=10^-5, l2=10^-4) sur les couches denses parce que le modÃ¨le avait du mal Ã  dÃ©passer 82% en exactitude sur les donnÃ©es de test Ã  cause du overfitting. Jâ€™ai gardÃ© l1 < l2 parce que lâ€™ajout des valeurs absolues des poids Ã  la perte tire les poids vers 0 plus fortement que lâ€™ajout des carrÃ©s des poids quand les poids sont proches de 0. Câ€™est ainsi, jâ€™ai rÃ©ussi Ã  atteindre une exactitude de 84.46%.

## ğŸ¯ RÃ©sultats

Exactitude : 84.46%

<img width="500" alt="image" src="https://github.com/user-attachments/assets/46bfa249-dec6-42cf-a54e-c3d0954ff38d" />

Un exemple par quadrant de la matrice de confusion (V pour "vÃ©ritÃ©", P pour "prÃ©diction"):

<img width="500" alt="image" src="https://github.com/user-attachments/assets/a3dd18ab-b03f-4c43-87d7-a168e4b8b0d0" />

Un quadrant noir signifie que mon modÃ¨le n'a pas produit d'exemple respectif.

## ğŸ“‹ Conclusion

Les difficultÃ©s quant au choix de lâ€™architecture sont discutÃ©es dans la sous-section Â« Justification du choix de lâ€™architecture Â» de la section "Montage de l'architecture et entrainement du modÃ¨le".

Notre limitation de temps et de ressources ne nous a pas permis d'explorer certaines avenues d'amÃ©lioration du modÃ¨le, dont celles-ci:
- Choisir plus judicieusement (par le biais d'un grid search?) les paramÃ¨tres de la rÃ©gularization l1 et l2
- ExpÃ©rimenter avec diffÃ©rentes architectures quant aux couches de convolution
